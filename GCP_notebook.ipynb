{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers fugashi unidic_lite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68c09e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying gs://jp-text-data/vocab.json...\n",
      "- [1 files][715.4 KiB/715.4 KiB]                                                \n",
      "Operation completed over 1 objects/715.4 KiB.                                    \n",
      "Copying gs://jp-text-data/merges.txt...\n",
      "/ [1 files][253.4 KiB/253.4 KiB]                                                \n",
      "Operation completed over 1 objects/253.4 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "DIR = 'jpRoberta'\n",
    "!mkdir $DIR\n",
    "!gsutil cp gs://jp-text-data/vocab.json $DIR/\n",
    "!gsutil cp gs://jp-text-data/merges.txt $DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaafe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merges.txt  vocab.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a51084",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://jp-text-data/jp-wiki-500k-sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "import torch\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        if is_tpu_available():\n",
    "            train_sampler = get_tpu_sampler(self.train_dataset)\n",
    "        else:\n",
    "            train_sampler = (\n",
    "                RandomSampler(self.train_dataset)\n",
    "                if self.args.local_rank == -1\n",
    "                else DistributedSampler(self.train_dataset)\n",
    "            )\n",
    "        data_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "            sampler=train_sampler if not isinstance(self.train_dataset, IterableDataset) else None,\n",
    "            collate_fn=self.data_collator.collate_batch,\n",
    "        )\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from robarta_japanese_tokenizer import RobertaJapaneseTokenizer\n",
    "\n",
    "tokenizer = RobertaJapaneseTokenizer.from_pretrained(DIR,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomIterableDataset(IterableDataset):\n",
    "  def __init__(self, filename, tokenizer, block_size, len):\n",
    "    self.filename = filename\n",
    "    self.tokenizer = tokenizer\n",
    "    self.block_size = block_size\n",
    "    self.len = len \n",
    "\n",
    "  def preprocess(self, text):\n",
    "    batch_encoding = self.tokenizer(text.strip(\"\\n\"), add_special_tokens=True, truncation=True, max_length=self.block_size)\n",
    "\n",
    "    return torch.tensor(batch_encoding[\"input_ids\"])\n",
    "\n",
    "  def line_mapper(self, line):      \n",
    "    return self.preprocess(line)\n",
    "\n",
    "  def __iter__(self):\n",
    "    file_itr = open(self.filename, encoding=\"utf-8\")\n",
    "    mapped_itr = map(self.line_mapper, file_itr)\n",
    "\n",
    "    return mapped_itr\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.len\n",
    "\n",
    "dataset = CustomIterableDataset(\"jp-wiki-500k-sample.txt\", tokenizer=tokenizer, block_size=256, len=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52000,\n",
    "    max_position_embeddings=512,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"jpBert\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    #max_steps = 2500,\n",
    "    warmup_steps = 500,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90733a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4168dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp jpRoberta/* gs://jp-text-data/jpRoberta/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
